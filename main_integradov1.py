# main_integrado.py
import sys
from langchain_openai import OpenAIEmbeddings
import ollama
import json
import matplotlib
from datetime import datetime
from dotenv import load_dotenv
from rapidfuzz import fuzz, process
from ragas.testset.generator import TestsetGenerator
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from utils.visualizacion import generar_pie_sentiment, generar_wordcloud


from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama

#from scrapers.scraper_youtube import ScraperYouTube
#from scrapers.scraper_redditV2 import ScraperReddit
from scrapers.scraper_yotubeV2 import ScraperYouTube
from scrapers.scraper_reddit import ScraperReddit

from rag.rag_manager import RAGManager
from scrapers.scraper_hibrido import ScraperHibrido
from utils.downloader import DescargadorInteligente
from processors.normalizador import NormalizadorMVP


# Importar tu sistema existente
sys.path.append('.')
# Cargar variables de entorno
load_dotenv()
matplotlib.use('Agg')

# ConfiguraciÃ³n del modelo Ollama
MODELO_OLLAMA = "llama3.1:8b"




def analizar_con_ollama(rag_manager, stats=None, query="percepciÃ³n general de Apple", modo="reporte", modelo=MODELO_OLLAMA):
    print("Buscando comentarios relevantes en RAG")
    docs = rag_manager.buscar_relevantes(query, k=100 if modo == "reporte" else 100)
    print(f"â†’ Recuperados {len(docs)} documentos (k=100)")

    # DeduplicaciÃ³n rÃ¡pida por contenido (elimina comentarios idÃ©nticos)
    unique_docs = []
    seen_texts = set()
    for doc in docs:
        text = doc.page_content.strip()
        if text not in seen_texts:
            seen_texts.add(text)
            unique_docs.append(doc)

    docs = unique_docs

    print(f"â†’ DespuÃ©s de dedup bÃ¡sico: {len(docs)} Ãºnicos")

    try:
        def dedup_fuzzy_retrieval(docs_list, threshold=88):
            texts = [d.page_content.strip() for d in docs_list]
            unique = []
            used = set()
            for i, txt in enumerate(texts):
                if i in used: continue
                unique.append(docs_list[i])
                if i + 1 < len(texts):
                    matches = process.extract(txt, texts[i+1:], scorer=fuzz.token_sort_ratio, limit=None)
                    for _, score, idx in matches:
                        if score >= threshold:
                            used.add(i + 1 + idx)
            return unique

        docs = dedup_fuzzy_retrieval(docs)
        print(f"â†’ DespuÃ©s de fuzzy en retrieval: {len(docs)} Ãºnicos")
    except:
        pass
    

    # DeduplicaciÃ³n semÃ¡ntica (elimina comentarios muy similares)
    try:
        embeddings = rag_manager.embeddings.embed_documents([d.page_content for d in docs])
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity

        unique_sem = []
        used_sem = set()
        for i, emb in enumerate(embeddings):
            if i in used_sem: continue
            unique_sem.append(docs[i])
            sims = cosine_similarity([emb], embeddings[i+1:])[0]
            for j, s in enumerate(sims):
                if s >= 0.90:  
                    used_sem.add(i + 1 + j)

        docs = unique_sem
        print(f"â†’ DespuÃ©s semÃ¡ntica (threshold 0.90): {len(docs)} Ãºnicos")
    except Exception as e:
        print(f"âš ï¸ SemÃ¡ntica fallÃ³: {e}")
        
        
    # Se usa Reranking con Cross-Encoder para mejorar relevancia
    try:
        from sentence_transformers import CrossEncoder
        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')  # Modelo ligero y efectivo

        # Pares: (query, doc_text)
        pairs = [[query, doc.page_content] for doc in docs]
        scores = reranker.predict(pairs)

        # Reordena por score descendente
        sorted_indices = np.argsort(scores)[::-1]
        docs = [docs[i] for i in sorted_indices]

        print(f"â†’ DespuÃ©s reranking: {len(docs)} docs reordenados por relevancia")
    except Exception as e:
        print(f"âš ï¸ Reranking fallÃ³: {e}")
  

    print(f"â†’ DespuÃ©s de deduplicaciÃ³n: {len(docs)} documentos Ãºnicos")

    # Limita a un nÃºmero razonable para Ollama (evita prompt demasiado largo)
    docs = docs[:150]
    if stats is None:
        stats = {"positivo": 0, "negativo": 0, "neutral": 0}
    if not docs:
        return {"error": "No se encontraron comentarios relevantes"}
    
    textos_comentarios = "\n\n".join([
    f"Comentario relevante #{i+1} (plataforma: {doc.metadata.get('plataforma', 'desconocida')}, fuente: {doc.metadata.get('fuente_tipo', 'desconocida')}):\n"
    f"{doc.page_content.strip()[:800]}\n"
    f"URL: {doc.metadata.get('url', 'N/A')}\n"
    for i, doc in enumerate(docs)
])
    
    fecha_actual = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    if modo == "reporte":
        prompt = f"""
    Eres un analista profesional de percepciÃ³n de marca.

    REGLAS:
    - Responde SOLO con JSON vÃ¡lido. Nada mÃ¡s. Empieza con {{ y termina con }}.
    - Usa SOLO los comentarios abajo.
    - Llena TODOS los campos aunque la evidencia sea dÃ©bil o minoritaria (usa "Algunos usuarios mencionan...", "Existe cierta crÃ­tica sobre...").
    - Si no hay suficiente para un campo, escribe "No hay evidencia clara" pero no lo dejes vacÃ­o.
    - Prioriza repeticiones, pero incluye tendencias aunque sean pocas.
    - No copies texto literal.

    JSON EXACTO:

    {{
    "fecha_analisis": "{fecha_actual}",
    "total_comentarios_analizados": {len(docs)},
    "percepcion_general": "frase resumen (obligatorio)",
    "sentimiento_general": {{ "positivo": {stats.get("positivo", 0)}, "negativo": {stats.get("negativo", 0)}, "neutral": {stats.get("neutral", 0)} }},
    "fortalezas": ["1 o mÃ¡s frases cortas (no dejar vacÃ­o si hay algo positivo)"],
    "debilidades": ["1 o mÃ¡s frases cortas (no dejar vacÃ­o si hay algo negativo)"],
    "problemas_frecuentes": ["problema 1 o 'ninguno claro'"],
    "tendencias_emergentes": ["tendencia 1 o 'ninguna clara'"],
    "resumen_ejecutivo": "2-4 oraciones (obligatorio, aunque sea breve)"
    }}

    Si no puedes: {{"error": "No se pudo generar"}}

    Comentarios:
    {textos_comentarios}
    """

    else:  # modo = "pregunta"
        prompt = f"""
    Eres un analista de percepciÃ³n de marca Apple.

    Tu respuesta DEBE basarse EXCLUSIVAMENTE en los comentarios recuperados del RAG.
    NO uses conocimiento externo ni generalices mÃ¡s allÃ¡ de lo que aparece en los textos.

    REGLAS ESTRICTAS:
    - Basate exclusivamente en los comentarios proporcionados abajo.
    - Si un tema tiene muchas menciones â†’ descrÃ­bela como patrÃ³n comÃºn ("muchos usuarios dicen...", "la mayorÃ­a percibe...")
    - Si aparece en pocos comentarios â†’ di "algunos usuarios mencionan..."
    - Si NO aparece o hay muy poca evidencia â†’ responde: "No hay evidencia suficiente en los comentarios analizados para concluir sobre este tema."
    - Puedes describir patrones con ejemplos genÃ©ricos (sin copiar texto literal completo): "varios usuarios se quejan del precio alto", "muchos destacan la fluidez del sistema".
    - NO inventes temas ni afirmaciones que no estÃ©n respaldadas por los comentarios.
    - Si el usuario pide comentarios textuales exactos â†’ di: "Por polÃ­tica de privacidad y para proteger el anonimato, no comparto citas textuales literales. Puedo describir patrones y tendencias generales."

    Comentarios disponibles ({len(docs)} recuperados para esta pregunta):
    {textos_comentarios}

    Pregunta del usuario: {query}

    Responde de forma clara, objetiva y fiel a los datos.
    NOTA IMPORTANTE SOBRE LA BASE DE DATOS:
    - En la base completa hay aproximadamente {rag_manager.get_total_documents()} comentarios recopilados.
    - AquÃ­ te estoy pasando solo los {len(docs)} mÃ¡s relevantes y Ãºnicos para esta pregunta.
    - Responde basado exclusivamente en los comentarios que te doy, pero puedes mencionar si el tema parece comÃºn o raro en el conjunto total.
    
    """


    try:
        response = ollama.generate(model=modelo, prompt=prompt)
        resultado_texto = response['response'].strip()

        import re
        resultado_texto = re.sub(r'^```json\s*|\s*```$', '', resultado_texto, flags=re.MULTILINE).strip()

        if modo == "reporte":
            json_match = re.search(r'\{.*\}', resultado_texto, re.DOTALL)
            json_str = json_match.group(0) if json_match else resultado_texto
            try:
                reporte = json.loads(json_str)
                if isinstance(reporte, list) and len(reporte) > 0:
                    reporte = reporte[0]
                # Normalizar estructura mÃ­nima
                reporte.setdefault("fecha_analisis", fecha_actual)
                reporte.setdefault("total_comentarios_analizados", len(docs))
                reporte.setdefault("percepcion_general", "No disponible")
                reporte.setdefault("sentimiento_general", {"positivo": 0, "negativo": 0, "neutral": 0})
                reporte.setdefault("fortalezas", [])
                reporte.setdefault("debilidades", [])
                reporte.setdefault("resumen_ejecutivo", "No disponible")

                print("âœ… Reporte JSON parseado correctamente")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ Error parseando JSON: {e}")
                reporte = {"error": "JSON invÃ¡lido", "texto_crudo": resultado_texto}
            
            # Guardar reporte
            filename = f"reporte_percepcion_apple_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(reporte, f, ensure_ascii=False, indent=2)
            print(f"âœ… Reporte guardado: {filename}")
            return reporte
        else:
            # Modo pregunta: respuesta en texto natural
            return {"respuesta_texto": resultado_texto}

    except Exception as e:
        print(f"âŒ Error con Ollama: {e}")
        return {"error": str(e)}

def inferir_fuente(item):
    fuente = (
        item.get("fuente")
        or item.get("url")
        or item.get("plataforma")
        or ""
    ).lower()

    if "youtube" in fuente:
        return "youtube", "red_social"
    if "reddit" in fuente:
        return "reddit", "foro"
    if "twitter" in fuente or "x.com" in fuente:
        return "x", "red_social"
    if "tiktok" in fuente:
        return "tiktok", "red_social"
    if "macrumors" in fuente:
        return "macrumors", "foro"
    if "forum" in fuente:
        return "foro", "foro"

    return "web", "blog"



def clasificar_texto(texto: str) -> str:
    """
    Clasificador mÃ¡s relajado y efectivo.
    Prioriza detectar cualquier opiniÃ³n (positiva o negativa) sobre informativo/ruido.
    """
    texto_l = texto.lower().strip()

    # 1. Umbral mÃ¡s bajo: comentarios de 15+ caracteres pueden ser vÃ¡lidos
    if len(texto_l) < 15:
        return "ruido"

    # Palabras clave fuertes de opiniÃ³n/queja (ampliamos mucho, espaÃ±ol + inglÃ©s)
    palabras_opinion_fuerte = [
        # EspaÃ±ol
        "me gusta", "no me gusta", "odio", "amo", "prefiero", "recomiendo",
        "vale la pena", "no vale la pena", "caro", "carÃ­simo", "barato",
        "increÃ­ble", "decepcionante", "frustrante", "problema", "bug",
        "fallo", "error", "defecto", "genial", "perfecto", "horrible",
        "cambiÃ©", "dejÃ©", "regreso", "nunca mÃ¡s", "para siempre",
        # InglÃ©s (por si hay comentarios mixtos)
        "love", "hate", "amazing", "disappointing", "overpriced", "worth it",
        "regret", "best", "worst", "issue", "bug", "problem"
    ]

    # Frases indicadoras de experiencia personal
    frases_personales = [
        "yo ", "mi iphone", "mi mac", "en mi caso", "para mÃ­", "en mi experiencia",
        "tengo un", "comprÃ©", "uso un", "mi experiencia", "despuÃ©s de usar",
        "llevo", "meses con", "aÃ±os con", "mi opiniÃ³n", "opino que"
    ]

    # Si tiene cualquiera de estas â†’ opiniÃ³n (positiva o negativa)
    if any(palabra in texto_l for palabra in palabras_opinion_fuerte):
        return "opinion"

    if any(frase in texto_l for frase in frases_personales):
        return "opinion"

    # Solo si parece puramente informativo lo marcamos como tal
    palabras_informativas = [
        "lanzamiento", "presentado", "caracterÃ­sticas", "especificaciones",
        "precio oficial", "disponible en", "se filtra", "rumor"
    ]
    if any(p in texto_l for p in palabras_informativas):
        return "informativo"

    # Por defecto: si pasa el filtro de longitud, lo consideramos opiniÃ³n potencial
    return "opinion"


def responder_con_stats(stats_globales, fuentes_stats):
    print("\nğŸ“Š MÃ‰TRICAS DE FUENTES")

    total = sum(fuentes_stats.values())

    for fuente, total_fuente in fuentes_stats.items():
        pct = round(total_fuente * 100 / total, 1) if total > 0 else 0
        print(f"- {fuente}: {total_fuente} ({pct}%)")

def es_opinion_real(texto: str) -> bool:
    """
    Filtro final ultra-relajado.
    Solo descarta si es claramente ruido o propaganda.
    """
    texto_l = texto.lower()

    # Umbral muy bajo: 12 caracteres mÃ­nimo
    if len(texto_l) < 12:
        return False

    # Descartar solo cosas muy obvias de ruido
    ruido_obvio = [
        "suscrÃ­bete", "dale like", "activa la campanita",
        "gracias por ver", "comentario fijado", "primer",
        "https://", "www.", "@", "giveaway", "sorteo"
    ]
    if any(r in texto_l for r in ruido_obvio):
        return False

    # Si tiene al menos una palabra emocional o personal â†’ opiniÃ³n real
    palabras_emocionales = [
        "bueno", "malo", "mejor", "peor", "caro", "barato",
        "gusta", "odio", "amo", "increÃ­ble", "horrible",
        "problema", "genial", "perfecto", "decepcion"
    ]
    if any(p in texto_l for p in palabras_emocionales):
        return True

    # Si menciona productos Apple directamente + verbo â†’ casi siempre opiniÃ³n
    productos = ["iphone", "mac", "ipad", "watch", "airpods", "ios", "macos", "apple"]
    verbos = ["es", "tiene", "funciona", "vale", "cuesta", "dura"]
    if any(prod in texto_l for prod in productos) and any(v in texto_l for v in verbos):
        return True

    # Por defecto: aceptamos (relajado mÃ¡ximo)
    return True



def main():
    global scraper
    global x_scraper
    print("""
    ========================================
    ğŸ¤– SISTEMA INTEGRADO: SCRAPING + OLLAMA + RAG (LOCAL)
    ========================================
    """)

    print("ğŸ”§ Inicializando componentes...")
    
    print("ğŸ“ Configurando RAGManager...")
    
    # === SOLO UNA INICIALIZACIÃ“N DEL RAG ===
    persist_dir = "./apple_sentiment_db"  # Directorio Ãºnico y consistente
    print(f"ğŸ“ Configurando RAGManager en: {persist_dir}")
    
    #CreaciÃ³n de memoria:
    llm = Ollama(model=MODELO_OLLAMA, temperature=0.1)
    # Memoria con resumen automÃ¡tico (mantiene ~2000-3000 tokens aprox)
    memory = ConversationSummaryBufferMemory(
        llm=llm,
        max_token_limit=3600,           # ajusta segÃºn tu hardware
        memory_key="chat_history",
        return_messages=True
    )
    question_prompt = PromptTemplate(
            input_variables=["chat_history", "question", "context"],
            template="""
        Eres un analista experto en percepciÃ³n de marca.
        Usa SOLO la informaciÃ³n de los comentarios recuperados y el historial de conversaciÃ³n.

        Historial de conversaciÃ³n anterior:
        {chat_history}

        Contexto recuperado del RAG (comentarios relevantes):
        {context}

        Pregunta actual del usuario: {question}

        Responde de forma clara, objetiva y fiel a los datos.
        Si el tema no aparece en el contexto ni en el historial â†’ di "No hay suficiente informaciÃ³n en los datos analizados".
        """
    )

    # Cadena para responder preguntas (con memoria)
    chain = (
        RunnablePassthrough.assign(
            chat_history=lambda x: memory.load_memory_variables({})["chat_history"]
        )
        | question_prompt
        | llm
        | StrOutputParser()
    )
    
    # Inicializa RAGManager UNA SOLA VEZ
    rag = RAGManager(persist_directory=persist_dir)
    print(f"âœ… RAG inicializado. Documentos actuales: {rag.get_total_documents()}")
    
    # Inicializa scrapers
    descargador = DescargadorInteligente(delay_min=4, delay_max=7)
    normalizador = NormalizadorMVP()
    scraper = ScraperHibrido(descargador, normalizador)
    youtube_scraper = ScraperYouTube()
    reddit_scraper = ScraperReddit()



    print("\nğŸš€ FASE 1: Web Scraping + YouTube")
    print("=" * 40)

    # INICIALIZAR lista vacÃ­a ANTES de cualquier scraping

    todos_datos = []

    # 1. Scraping web tradicional
    urls = [
    # Blogs con comunidad activa (comentarios)
 


    # Foros (ORO PURO)
    
    "https://forums.macrumors.com/forums/iphone.109/",
    "https://forums.macrumors.com/forums/macbook.115/",
    "https://forums.macrumors.com/forums/apple-watch.130/",
    "https://forums.macrumors.com/forums/ipad.122/",
]


    for i, url in enumerate(urls, 1):
        print(f"\nğŸ“„ [{i}/{len(urls)}] Scrapeando web: {url}")
        datos = scraper.scrape(url)
        if datos:
            todos_datos.extend(datos)
            print(f"   âœ… ExtraÃ­dos {len(datos)} elementos web")
   
    
   
    # 2. Scraping YouTube
    print("\nğŸš€ FASE 1.5: Scraping YouTube Comments")
    print("=" * 40)

    youtube_keywords = [
    # Reviews reales
    "iPhone 16 review espaÃ±ol",
    "iPhone 16 opiniÃ³n sincera",
    "MacBook Pro M4 review",
    "Apple Watch Series 10 opiniones",
    "iPhone 16 problemas reales",
    "iOS 18 bugs espaÃ±ol",
    "Apple caro vale la pena",
    "por quÃ© dejÃ© Apple",
    "ecosistema Apple atrapado",
    "MacBook M4 fallos",
    "Apple no innova 2025"
    # Experiencia real
    "iPhone 16 despuÃ©s de un mes",
    "MacBook Pro M4 uso profesional",
    "Apple ecosistema experiencia real",

    # Unboxing / emociÃ³n
    "unboxing iPhone 16 espaÃ±ol",
    "AirPods Pro 2 unboxing",
    "primera impresiÃ³n Apple Vision Pro",

    # Problemas reales
    "iOS 18 problemas reales",
    "iPhone 16 problemas comunes",
    "iCloud problemas usuarios",

    # Comparaciones
    "iPhone vs Samsung experiencia",
    "MacBook vs Windows experiencia real",

    # Servicios
    "Apple Music vs Spotify opiniÃ³n",
    "Apple TV Plus opiniones reales",

    # Marca / percepciÃ³n
    "por quÃ© la gente ama Apple",
    "Apple sobrevalorado opiniÃ³n",
    "Apple caro vale la pena"
]


    comentarios_yt = youtube_scraper.scrape_comentarios_keywords(
        keywords_list=youtube_keywords,
        max_videos_per_kw=4,          # 8 videos por keyword
        max_comments_per_video=10     # Hasta 100 comentarios por video
    )

    todos_datos.extend(comentarios_yt)

    total_yt = len(comentarios_yt)
    print(f"\nâœ… Total comentarios extraÃ­dos de YouTube: {total_yt}")

    if total_yt == 0:
        print("\nâš ï¸ No se extrajeron comentarios de YouTube. Continuando con otras fuentes...")
    else:
        print(f"   Promedio aproximado por keyword: {total_yt // len(youtube_keywords)} comentarios")

    # ContinÃºa con el resto
    print(f"\nğŸ’¾ Total elementos scrapeados hasta ahora: {len(todos_datos)} (web + YouTube)")
    print("\nğŸš€ FASE 1.6: Scraping TikTok Comments")
    print("=" * 40)


    """
    #Aun no se utiliza debido a limitaciones con la Api de TikTok y bloqueos frecuentes.
    print("\nğŸš€ FASE 1.6: Scraping TikTok Comments (unofficial API)")
    print("=" * 40)

    tiktok_keywords = [
        "iPhone 16 review espaÃ±ol",
        "AirPods Pro 2 unboxing",
        "MacBook Pro M4 review",
        "Apple Watch Series 10 opiniones",
        "iOS 18 problemas",
        "Apple Vision Pro anÃ¡lisis"
    ]

    tiktok_comentarios = tiktok_scraper.scrape_comments_keywords(tiktok_keywords, max_videos=2, max_comments_per_video=15)
    todos_datos.extend(tiktok_comentarios)
    tiktok_scraper.cerrar()
 
    """
    print("\nğŸš€ FASE REDDIT: Scraping Opiniones")
    print("=" * 40)

    subreddits_queries = {
    # NÃºcleo Apple (opiniÃ³n + crÃ­tica)
    "apple": (
        "Apple experience OR Apple ecosystem worth it OR "
        "Apple overpriced OR Apple disappointed OR leaving Apple"
    ),

    # iPhone (uso real + problemas)
    "iphone": (
        "iPhone experience OR iPhone problems OR "
        "iPhone regret OR iPhone not worth it"
    ),

    # Mac / macOS
    "mac": (
        "MacBook experience OR macOS issues OR "
        "MacBook overpriced OR MacBook regret"
    ),

    # iOS bugs / frustraciÃ³n
    "ios": (
        "iOS problems OR iOS bugs OR "
        "iOS frustrating OR iOS updates broke"
    ),

    # Soporte / quejas directas
    "applehelp": (
        "Apple problem OR Apple issue OR "
        "Apple support bad OR Apple not helping"
    ),

    # OpiniÃ³n general / consumo
    "gadgets": (
        "Apple review OR Apple opinion OR "
        "Apple overpriced OR Apple not worth it"
    ),

    # ComparaciÃ³n / abandono
    "android": (
        "Apple vs Android OR switching from Apple OR leaving Apple ecosystem"
    ),

    # ComparaciÃ³n profesional
    "windows": (
        "MacBook vs Windows OR leaving MacBook OR switch from Mac"
    )
}




    for sub, query in subreddits_queries.items():
        print(f"   ğŸ” Scrapeando r/{sub}")
        datos = reddit_scraper.scrape_subreddit(
            subreddit=sub,
            query=query,
            limit=100,  # MÃ¡ximo 100 posts por subreddit
        )
        if datos:
            todos_datos.extend(datos)
            print(f"   âœ… {len(datos)} comentarios extraÃ­dos de r/{sub}")
        else:
            print(f"   âš ï¸ Sin resultados en r/{sub}")


   

    print(f"   Total acumulado hasta ahora: {len(todos_datos)} elementos")
   
    unique = {item.get('texto', '').strip(): item for item in todos_datos if item.get('texto')}
    todos_datos = list(unique.values())
    print(f"DespuÃ©s de dedup exacta: {len(todos_datos)} elementos")

    # Ahora la fuzzy (la importante)
 

    def deduplicar_fuzzy(items, threshold=90):
        textos = [item.get('texto', '').strip() for item in items]
        unique = []
        usados = set()

        for i, texto in enumerate(textos):
            if i in usados or not texto:
                continue
            unique.append(items[i])
            if i + 1 < len(textos):
                matches = process.extract(
                    texto,
                    textos[i+1:],
                    scorer=fuzz.token_sort_ratio,
                    limit=None
                )
                for _, score, rel_idx in matches:
                    if score >= threshold:
                        usados.add(i + 1 + rel_idx)

        print(f"DeduplicaciÃ³n fuzzy ({threshold}%): {len(items)} â†’ {len(unique)} elementos")
        return unique

     
    todos_datos = deduplicar_fuzzy(todos_datos, threshold=90)
    print(f"Total despuÃ©s de fuzzy + exacta: {len(todos_datos)} elementos")
    print(f"ReducciÃ³n por fuzzy: {len(unique) - len(todos_datos)} elementos eliminados")
    """
    # Scraper Instagram
    print("\nğŸš€ FASE 1.8: Scraping Instagram Comments (perfiles pÃºblicos)")
    print("=" * 40)

    # Perfiles pÃºblicos con contenido Apple (unboxings, reviews, fans)
    ig_profiles = [
    "mkbhd", 
        ]

    for profile in ig_profiles:
        comments = ig_scraper.scrape_comments_profile(profile, max_posts=1, max_comments_per_post=5)
        todos_datos.extend(comments)
        time.sleep(120)  # Delay entre perfiles para evitar bloqueo total

    # Opcional: perfil oficial de Apple (pÃºblico)
    comments_apple = ig_scraper.scrape_comments_profile("apple", max_posts=3, max_comments_per_post=20)
    todos_datos.extend(comments_apple)
    
    
    
    
    """
    plataforma_stats = {
    "reddit": 0,
    "youtube": 0,
    "web": 0,
    "x": 0
    }

    stats_pipeline = {
    "total_scrapeados": 0,
    "descartados_ruido": 0,
    "descartados_cortos": 0,
    "validos": 0
    }

    stats_globales = {
    "opinion": 0,
    "informativo": 0
    }

    fuentes_stats = {}
    """
    # DeduplicaciÃ³n por texto exacto
    unique = {item.get('texto', '').strip(): item for item in todos_datos if item.get('texto')}
    def deduplicar_mejorado(items, threshold=92):
        textos = [item.get('texto','').strip() for item in items]
        unique = []
        indices_usados = set()

        for i, texto in enumerate(textos):
            if i in indices_usados:
                continue
            unique.append(items[i])
            # Buscar similares
            matches = process.extract(texto, textos[i+1:], scorer=fuzz.token_sort_ratio)
            for match, score, idx in matches:
                if score >= threshold:
                    indices_usados.add(i + 1 + idx)

        return unique

    # Uso
    todos_datos = deduplicar_mejorado(todos_datos, threshold=90)
    """
    print(f"DespuÃ©s de dedup: {len(todos_datos)} elementos")
    comentarios_validos = []
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #          GUARDAR TODOS LOS DATOS SCRAPEADOS PARA INSPECCIÃ“N
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    print("\nğŸ’¾ Guardando datos scrapeados crudos para revisiÃ³n...")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. VersiÃ³n JSON completa y bonita (fÃ¡cil de abrir con editor o navegador)
    json_path = f"datos_scrapeados_crudos_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(todos_datos, f, ensure_ascii=False, indent=2, default=str)
    print(f"   â†’ JSON completo: {json_path}  ({len(todos_datos):,} elementos)")

    # 2. VersiÃ³n texto plano muy legible (ideal para revisar con ojos humanos)
    txt_path = f"datos_scrapeados_crudos_{timestamp}.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(f"ARCHIVO DE DATOS SCRAPEADOS - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total elementos scrapeados: {len(todos_datos)}\n")
        f.write("=" * 80 + "\n\n")

        for i, item in enumerate(todos_datos, 1):
            f.write(f"â”Œâ”€â”€â”€ ELEMENTO #{i} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
            
            # Texto principal (el mÃ¡s importante)
            texto = item.get("texto") or item.get("contenido") or item.get("text") or "(sin texto)"
            f.write(f"TEXTO:\n{texto}\n\n")
            
            # Metadatos clave
            campos_interesantes = [
                ("Plataforma", item.get("plataforma") or item.get("source") or "â€”"),
                ("URL", item.get("url") or item.get("video_url") or "â€”"),
                ("Autor", item.get("autor") or item.get("author") or item.get("username") or "â€”"),
                ("Fecha", item.get("fecha") or item.get("date") or item.get("published_at") or "â€”"),
                ("Tipo", item.get("tipo") or "â€”")
            ]
            
            for nombre, valor in campos_interesantes:
                if valor and valor != "â€”":
                    f.write(f"{nombre:12}: {valor}\n")
            
            f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n")

    print(f"   â†’ TXT legible: {txt_path}")
    print("   Â¡Listo! Abre cualquiera de los dos archivos para revisar los datos crudos.\n")
    
    
   
        
    
    for item in todos_datos:
        stats_pipeline["total_scrapeados"] += 1

        texto = item.get("texto") or item.get("contenido") or item.get("text") or ""
        texto = texto.strip().lower()

        # Filtro 1: VacÃ­os/cortos
        if not texto or len(texto) < 30:
            stats_pipeline["descartados_cortos"] += 1
            continue

        # Filtro 2: Ruido de headers/menÃºs (patrones especÃ­ficos de MacRumors/foros)
        ruido_patterns = [
            "got a tip for us", "send us an email", "anonymous form", "front page", "roundups",
            "airpods", "iphone", "macbook", "watchos", "search everywhere", "new posts", "forum list"
        ]
       # if any(pattern in texto for pattern in ruido_patterns) and len(texto) < 500:  # Solo si no es comentario largo
        #    stats_pipeline["descartados_ruido"] += 1
         #   continue

        # Filtro 3: Repeticiones (baja diversidad de palabras)
        palabras = texto.split()
        #if len(palabras) > 0 and len(set(palabras)) / len(palabras) < 0.4:  # <40% Ãºnicas â†’ repetitivo
         #   stats_pipeline["descartados_ruido"] += 1
          #  continue

        # Tus filtros existentes (clasificar_texto, es_opinion_real, etc.)
        tipo = clasificar_texto(texto)
        """ 
        if tipo == "ruido":
            stats_pipeline["descartados_ruido"] += 1
            continue
        """
  

        # ğŸ”¹ Inferir plataforma ANTES de usarla
        plataforma, fuente_tipo = inferir_fuente(item)
        item["plataforma"] = plataforma
        item["fuente_tipo"] = fuente_tipo

        # ğŸ”¹ Conteo por plataforma (TODOS, incluso descartados)
        plataforma_stats[plataforma] = plataforma_stats.get(plataforma, 0) + 1

        # ğŸ”¹ Filtros
       # if len(texto) <  30:  # MÃ¡s permisivo
        #    stats_pipeline["descartados_cortos"] += 1
         #   continue

        tipo = clasificar_texto(texto)
        if tipo == "ruido":
            stats_pipeline["descartados_ruido"] += 1
            continue

        # ğŸ”¹ VÃ¡lidos
        item["tipo"] = tipo
        stats_pipeline["validos"] += 1

        if tipo == "opinion" and es_opinion_real(texto):  # Quitamos "opinion_negativa" restrictivo
            stats_globales["opinion"] += 1
            item["tipo_fuente"] = "opinion_real"
            item["longitud_texto"] = len(texto)
            comentarios_validos.append(item)
        else:
            stats_globales["informativo"] += 1


        # ğŸ”¹ Conteo para preguntas tipo â€œReddit vs YouTubeâ€
        fuentes_stats[plataforma] = fuentes_stats.get(plataforma, 0) + 1

    total = sum(plataforma_stats.values())

    for plataforma, count in plataforma_stats.items():
        pct = round(count * 100 / total, 1) if total > 0 else 0
        print(f"{plataforma}: {count} ({pct}%)")

     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   Preparar datos para Ragas (dataset de evaluaciÃ³n inicial)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    print("\nğŸ“Š Preparando muestra para evaluaciÃ³n con Ragas...")

    # Usamos comentarios vÃ¡lidos (despuÃ©s de filtrado) para mayor calidad
    # Si quieres usar TODO lo crudo, cambia a: datos_para_ragas = todos_datos[:800]
    datos_para_ragas = comentarios_validos[:800]  # lÃ­mite razonable para no gastar demasiado

    if datos_para_ragas:
        ragas_samples = []
        for item in datos_para_ragas:
            texto = item.get("texto") or item.get("contenido") or item.get("text") or ""
            if len(texto.strip()) > 30:
                ragas_samples.append({
                    "text": texto,
                    "source": item.get("plataforma", "unknown"),
                    "url": item.get("url") or item.get("video_url") or "N/A",
                    "date": item.get("date") or item.get("fecha") or "N/A"
                })

        ragas_output = {
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_original_scraped": len(todos_datos),
            "total_after_filter": len(comentarios_validos),
            "sample_for_ragas": ragas_samples
        }

        ragas_file = f"ragas_ready_sample_{timestamp}.json"
        with open(ragas_file, "w", encoding="utf-8") as f:
            json.dump(ragas_output, f, ensure_ascii=False, indent=2)

        print(f"   â†’ Archivo para Ragas creado: {ragas_file}")
        print(f"      Contiene {len(ragas_samples)} comentarios listos para generaciÃ³n sintÃ©tica.")
    else:
        print("   No hay suficientes comentarios vÃ¡lidos para preparar Ragas aÃºn.")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   (Opcional) Generar testset sintÃ©tico con Ragas ahora mismo
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.documents import Document
        print("\nğŸ§ª Generando testset sintÃ©tico con Ragas (puede tardar)...")

        # Convertir a LangChain Documents
        lc_docs = [
            Document(
                page_content=item["text"],
                metadata={"source": item["source"], "url": item["url"]}
            )
            for item in ragas_samples[:100]  # lÃ­mite para no gastar mucho
        ]

        generator = TestsetGenerator.from_langchain(
            generator_llm=ChatOpenAI(model="gpt-4o-mini"),
            critic_llm=ChatOpenAI(model="gpt-4o-mini"),
            embeddings=OpenAIEmbeddings()
        )

        testset = generator.generate_with_langchain_docs(
            lc_docs,
            testset_size=10,  # 30-50 es buen comienzo
        )

        testset.to_pandas().to_csv(f"ragas_testset_{timestamp}.csv", index=False)
        print(f"   â†’ Testset generado y guardado: ragas_testset_{timestamp}.csv")

    except ImportError:
        print("   Ragas o dependencias no instaladas â†’ omite generaciÃ³n automÃ¡tica.")
    except Exception as e:
        print(f"   Error al generar testset con Ragas: {e}")
            
        
        
    total = stats_globales["opinion"] + stats_globales["informativo"]

    if total > 0:
        stats_globales["opinion_pct"] = round(stats_globales["opinion"] * 100 / total)
        stats_globales["informativo_pct"] = round(stats_globales["informativo"] * 100 / total)
    else:
        stats_globales["opinion_pct"] = 0
        stats_globales["informativo_pct"] = 0
    

    print("ğŸ“Š Agregando comentarios vÃ¡lidos al RAG...")
    rag.agregar_comentarios(comentarios_validos)
    print(f"âœ… Comentarios vÃ¡lidos procesados: {len(comentarios_validos)}")
        
        # VERIFICACIÃ“N CRÃTICA:
    print(f"\nğŸ” VERIFICACIÃ“N RAG:")
    print(f"   - Comentarios procesados: {len(comentarios_validos)}")
    print(f"   - Documentos en RAG: {rag.get_total_documents()}")
    print(f"   - Guardado en: {rag.persist_directory}")
        
        # Forzar guardado explÃ­cito
    if hasattr(rag, 'guardar'):
        print("\nğŸ’¾ Guardando RAG en disco...")
        rag.guardar()
     # Verificar fÃ­sicamente si existe
    import os
    if os.path.exists(rag.persist_directory):
        files = os.listdir(rag.persist_directory)
        print(f"   - Archivos en directorio: {len(files)} archivos")
        for f in files[:5]:
            size = os.path.getsize(os.path.join(rag.persist_directory, f))
            print(f"     â€¢ {f} ({size:,} bytes)")
    else:
        print(f"   âŒ Directorio no existe: {rag.persist_directory}")
    # Verificar persistencia creando nueva instancia
    print("\nğŸ§ª Probando persistencia...")
   
    rag_test = RAGManager(persist_directory=rag.persist_directory)
    print(f"   Documentos en nueva instancia: {rag_test.get_total_documents()}")


        # === NUEVO: EstadÃ­sticas de sentiment y grÃ¡ficos ===
    print("\nğŸ“Š Calculando estadÃ­sticas de sentiment y generando grÃ¡ficos...")
    stats = rag.get_sentiment_stats()  # â† Usa el nuevo mÃ©todo de rag_manager
    print(f"Sentiment global: Positivo {stats['positivo']}%, Negativo {stats['negativo']}%, Neutral {stats['neutral']}%")

    # Generar pie chart
    generar_pie_sentiment(stats, filename="sentiment_pie_apple.png")

    # Generar wordcloud (opcional, pero recomendado)
    docs_relevantes = rag.buscar_relevantes(
    "Apple iPhone Mac iOS servicios ecosistema",
    k=120
    )


    textos = [doc.page_content for doc in docs_relevantes]

    generar_wordcloud(textos, filename="wordcloud_apple.png")

    print("âœ… GrÃ¡ficos generados: sentiment_pie_apple.png y wordcloud_apple.png")
    # ==================================================
    
    
    print("\nğŸ§  FASE 2: AnÃ¡lisis con Ollama + RAG")
    print("=" * 40)
    print(f"ğŸ“„ Documentos enviados a Ollama: {len(docs_relevantes)}")
    query_reporte = (
    "experiencia uso Apple ecosistema opinion problema me gusta no me gusta "
    "iPhone Mac iOS macOS iCloud AirPods Apple Watch Vision Pro"
)


    reporte = analizar_con_ollama(
        rag,
        stats=stats,
        query=query_reporte,
        modo="reporte"
    )


    total = len(todos_datos)
    opiniones = sum(1 for c in todos_datos if c.get("tipo") == "opinion")
    informativos = total - opiniones


    if total == 0:
        print("âš ï¸ No hay opiniones reales suficientes para mÃ©tricas")
    else:
        opiniones = sum(
            1 for c in comentarios_validos 
            if c["tipo"] in ["opinion", "opinion_negativa"]
        )
        informativos = total - opiniones

        print(f"""
        ğŸ“Š MÃ‰TRICAS DE FUENTES
        OpiniÃ³n real: {opiniones} ({opiniones*100//total}%)
        Informativo: {informativos} ({informativos*100//total}%)
        """)



    if isinstance(reporte, dict):

        print("\nğŸ“Š RESUMEN DEL REPORTE")
        print("=" * 60)
        if 'error' in reporte:
            print("âš ï¸ Error en generaciÃ³n: ", reporte['error'])
            if 'texto_crudo' in reporte:
                print("\nTexto crudo generado por el modelo (primeros 2000 chars):")
                print(reporte['texto_crudo'][:2000])
        else:
            print(f"Fecha: {reporte.get('fecha_analisis', 'N/A')}")
            print(f"Comentarios analizados: {reporte.get('total_comentarios_analizados', 'N/A')}")
            print(f"\nPercepciÃ³n general:\n{reporte.get('percepcion_general', 'No disponible')}")
            sentimiento = reporte.get('sentimiento_general', {})
            print(f"\nSentimiento: Positivo {sentimiento.get('positivo', 0)}% | Negativo {sentimiento.get('negativo', 0)}% | Neutral {sentimiento.get('neutral', 0)}%")
            print(f"\nFortalezas: {', '.join(reporte.get('fortalezas', [])[:5])}")
            print(f"Debilidades: {', '.join(reporte.get('debilidades', [])[:5])}")
            print(f"\nResumen ejecutivo:\n{reporte.get('resumen_ejecutivo', 'No disponible')}")
    else:
        print("\nâš ï¸ No se generÃ³ reporte.")

    
    print("\n" + "="*60)
    print("ğŸ’¬ MODO INTERACTIVO (CON MEMORIA de conversaciÃ³n)")
    print("="*60)
    print("Puedes preguntar sobre percepciÃ³n de marca. Ejemplos:")
    print("  - Â¿QuÃ© opinan del iPhone 16?")
    print("  - Â¿Ha mejorado la percepciÃ³n de la baterÃ­a?")
    print("  - porcentaje de sentimiento positivo")
    print("Escribe 'salir' para terminar\n")

    while True:
        pregunta = input("\nğŸ¤” Tu pregunta: ").strip()
        
        if pregunta.lower() in ['salir', 'exit', 'quit', 'q']:
            print("\nğŸ‘‹ Â¡Hasta luego! La memoria de esta sesiÃ³n se perderÃ¡ al cerrar.")
            break
        
        if not pregunta:
            print("âš ï¸ Escribe una pregunta vÃ¡lida.")
            continue
        
        pregunta_lower = pregunta.lower()
        
        # 1. Preguntas cuantitativas â†’ responden con stats precalculadas (sin LLM)
        if any(p in pregunta_lower for p in [
            "porcentaje", "porcentajes", "cuÃ¡ntos", "cuantas", "cantidad",
            "distribuciÃ³n", "proporciÃ³n", "%", "cuÃ¡nto", "cuÃ¡nta"
        ]):
            print("\nğŸ“Š Respondiendo con estadÃ­sticas calculadas...")
            responder_con_stats(stats_globales, fuentes_stats)
            continue
        
        # 2. Preguntas sobre ruido / pipeline (debug)
        elif any(p in pregunta_lower for p in ["ruido", "descartados", "filtrado", "filtro"]):
            print(f"""
            ğŸ“‰ MÃ©tricas del pipeline de limpieza:
            â€¢ Comentarios descartados por ruido:      {stats_pipeline.get("descartados_ruido", 0)}
            â€¢ Comentarios demasiado cortos:           {stats_pipeline.get("descartados_cortos", 0)}
            â€¢ Total scrapeados inicialmente:          {stats_pipeline.get("total_scrapeados", 0)}
            â€¢ Comentarios vÃ¡lidos despuÃ©s de filtros: {stats_pipeline.get("validos", 0)}
            """)
            continue
        
        # 3. Todo lo demÃ¡s â†’ va a la IA con RAG + memoria
        print("\nğŸ¤– Analizando con RAG + memoria + Ollama...", end="", flush=True)
        
        # Recuperamos contexto relevante (k mÃ¡s bajo = mÃ¡s rÃ¡pido)
        docs = rag.buscar_relevantes(pregunta, k=70)   # 30-40 es un buen balance ahora
        
        if not docs:
            context_text = "No se recuperaron comentarios relevantes para esta pregunta."
        else:
            context_text = "\n\n".join([
                f"[{i+1}] {doc.page_content[:450]}... "
                f"(plataforma: {doc.metadata.get('plataforma', 'n/a')}, "
                f"fecha: {doc.metadata.get('fecha', 'n/a')})"
                for i, doc in enumerate(docs)
            ])
        
        try:
            respuesta = chain.invoke({
                "question": pregunta,
                "context": context_text
            })
            
            print("\râœ… Respuesta generada:")
            print(respuesta.strip())
            print("-"*80)
            
            # Guardar en memoria
            memory.save_context(
                {"input": pregunta},
                {"output": respuesta}
            )
            
        except Exception as e:
            print("\râŒ Error al generar respuesta:")
            print(str(e))
            print("Posibles causas: Ollama no estÃ¡ corriendo, prompt mal configurado o memoria saturada.")
            continue

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Programa interrumpido por el usuario")
    except Exception as e:
        print(f"\nâŒ Error crÃ­tico: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'scraper' in locals() and hasattr(scraper, 'cerrar_selenium'):
            scraper.cerrar_selenium()
     