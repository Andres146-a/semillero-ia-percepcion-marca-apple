# scrapers/scraper_hibrido.py
from .scraper_patrones import ScraperConPatrones
from .scraper_heuristicas import HeuristicasBasicas
from detectors.detector_tipo import DetectorTipoPagina
from processors.normalizador import NormalizadorMVP
from .scraper_selenium import ScraperSelenium
import urllib.parse
from .scraper_selenium import ScraperSelenium

class ScraperHibrido:
    """
    ‚úÖ NUEVO: Orquestador que combina patrones + heur√≠sticas
    Decide autom√°ticamente qu√© estrategia usar
    """
    
    def __init__(self, descargador, normalizador=None):
        self.descargador = descargador
        self.scraper_selenium = None
        self.scraper_patrones = ScraperConPatrones(descargador)
        self.scraper_heuristicas = HeuristicasBasicas(descargador)
        self.detector = DetectorTipoPagina()
        self.normalizador = normalizador or NormalizadorMVP()
    
    def scrape(self, url):
            """
            Flujo principal de scraping h√≠brido + Selenium fallback
            """
            print(f"\nüîç Iniciando scraping de: {url}")
            
            datos_normalizados = []
            
            # PASO 1: Intentar con patrones conocidos (m√°s r√°pido)
            if self.scraper_patrones.puede_manejar(url):
                print("‚úÖ Usando estrategia: PATR√ìN CONOCIDO")
                datos_crudos = self.scraper_patrones.scrape(url)
                tipo_fuente = self._obtener_tipo_patron(url)
                
                if datos_crudos:
                    datos_normalizados = self.normalizador.normalizar(datos_crudos, tipo_fuente, url)
                    print(f"   ‚úÖ Patrones extrajeron {len(datos_normalizados)} elementos normalizados")
                    
                    # Si sac√≥ al menos 8 elementos con texto decente ‚Üí √©xito, retornamos
                    if len(datos_normalizados) >= 20 and any(len(d.get('contenido', '')) > 100 for d in datos_normalizados):
                        return datos_normalizados
            
            # PASO 2: Si patrones fallaron o dieron poco ‚Üí heur√≠sticas con BeautifulSoup
            print("üîÑ Usando estrategia: HEUR√çSTICAS BS4")
            datos_crudos = self.scraper_heuristicas.scrape(url)
            tipo_fuente = self._inferir_tipo_fuente(url, datos_crudos or [])
            
            if datos_crudos:
                datos_normalizados = self.normalizador.normalizar(datos_crudos, tipo_fuente, url)
                print(f"   ‚úÖ Heur√≠sticas extrajeron {len(datos_normalizados)} elementos normalizados")
                
                # Si sac√≥ buen contenido ‚Üí √©xito
                if len(datos_normalizados) >= 10 and any(len(d.get('contenido', '')) > 80 for d in datos_normalizados):
                    return datos_normalizados
            
            # PASO 3: Fallback final ‚Üí Selenium (para JS din√°mico)
            print("üöÄ Activando fallback: SELENIUM (carga JavaScript completo)")
            if self.scraper_selenium is None:
                print("   Inicializando driver de Chrome...")
                self.scraper_selenium = ScraperSelenium(headless=True)
            
            datos_crudos_selenium = self.scraper_selenium.scrape(url, max_items=25)
            
            if datos_crudos_selenium:
                # Usamos tipo gen√©rico o inferido
                tipo_fuente = 'dinamico_js'
                datos_normalizados = self.normalizador.normalizar(datos_crudos_selenium, tipo_fuente, url)
                print(f"   ‚úÖ Selenium extrajo {len(datos_normalizados)} elementos ricos")
                return datos_normalizados
            else:
                print("   ‚ö†Ô∏è Selenium tampoco pudo extraer datos √∫tiles")
            
            # Si todo fall√≥
            print("‚ùå No se pudieron extraer datos √∫tiles de ning√∫n m√©todo")
            return []
    
    def _obtener_tipo_patron(self, url):
        """Obtiene el tipo de fuente desde los patrones configurados"""
        dominio = self.extraer_dominio(url)
        if hasattr(self.scraper_patrones, 'patrones') and dominio in self.scraper_patrones.patrones:
            return self.scraper_patrones.patrones[dominio].get('tipo', 'desconocido')
        return 'patron_conocido'
    
    def _inferir_tipo_fuente(self, url, datos_crudos):
        """Infiere el tipo de fuente cuando no hay patr√≥n"""
        # Preguntar al detector
        dominio = self.extraer_dominio(url)
        
        # Reglas simples basadas en dominio
        if any(palabra in dominio for palabra in ['foro', 'forum', 'board']):
            return 'foro'
        elif any(palabra in dominio for palabra in ['blog', 'medium', 'substack']):
            return 'blog'
        elif any(palabra in dominio for palabra in ['shop', 'store', 'tienda', 'amazon']):
            return 'ecommerce'
        elif any(palabra in dominio for palabra in ['reddit', 'twitter', 'tiktok']):
            return 'red_social'
        
        # Si no se identifica, usar 'desconocido'
        return 'desconocido'
    
    def extraer_dominio(self, url):
        """Extrae el dominio de una URL"""
        parsed = urllib.parse.urlparse(url)
        return parsed.netloc.replace('www.', '')    
    
    def cerrar_selenium(self):
        """Cierra el driver de Selenium si est√° activo"""
        if self.scraper_selenium:
            print("üîª Cerrando navegador Selenium...")
            self.scraper_selenium.cerrar()
            self.scraper_selenium = None